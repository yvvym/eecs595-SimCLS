{"article": ["if you're asking about absolute answers for specific deeds:   no.", "what about (and i'm borrowing this from a former philosophy prof.), \"it's wrong to torture infants for fun,\" (and feel free to replace 'for fun' with 'for enjoyment' or 'to alleviate pain' or some other such thing).", "well, there's a logically possible world wherein the only way to have fun is by torturing infants.", "the infants certainly don't enjoy this, otherwise it's not torture.", "however  in this logically possible world, the only way one can have fun at all (only by torturing infants), is if one was already tortured as an infant.", "the amount of fun one can have as an adult is proportional to the amount of torture endured as an infant.", "in this logically possible world, the fun gained by torturing infants is  extreme  enough and consistent enough to outweigh any act- or rule-utilitarian suspicion that torturing the infants is wrong.", "further, one function of the torturing an individual is to facilitate their later ability to have fun, so you aren't treating the infants as a mere means.", "if the rational members of this society went behind  rawls's veil of ignorance  about having been tortured, so this passes a casual \"golden rule\" test.", "my point:  if your candidate absolute basis for morality gives absolute recommendations on specific deeds, even if it's something like \"it's wrong to torture infants for fun,\" there's a logically possible world that contradicts your candidate... it's not absolute.", "but note the standards i used for exploring the infant-torturing thought experiment... utilitarianism, kant-style deontology, the veil of ignorance, and the casual \"golden rule\".", "these have far more flexibility... any specific recommendation on a deed is context-dependent, but based outside of that context.", "if context gets weird enough, so do the specific recommendations.", "so, where does that leave us?", "well, i'm partial to rawls's veil of ignorance for \"big\" (society-wide) moral matters.", "in  this  world, regarding humans is slavery just?", "well, since we're starting with humans in  this  world, we don't need to worry about weirdness like \"maybe they like being slaves\".", "we know they don't, because we know how humans work, their basic likes and dislikes.", "so, if everyone gets behind the veil of ignorance, not knowing whether they're a slave or not, would they collectively decide that a society with slavery - one where they themselves might be slaves - is just?", "i think not.", "this (contra-slavery result) works for the other approaches i mentioned: in case you want to do a utilitarian calculation, we have a fair sense of the array of consequences of slavery.", "it's wrong.", "of course, if you change the context enough... if you put forth a sufficiently weird enough, but logically possible world, the specific answer to \"[in that logically possible world] is slavery wrong\" may differ.", "for less \"obvious\" specific recommendations, where the intuitions differ from one culture to the next (oh... i don't know... \"is it morally permissible to divorce a non-abusive spouse?\")", "you won't necessarily get a \"result\" from kant-style deontology, or utilitarianism, or rawls, from the armchair... at least not until you collect more information - enough to properly run the utilitarian calculation, imagine that society behind the veil of ignorance, or whatever.", "even then, it's probably ok for a basis for morality to be silent on some issues... it can be absolute without being all encompassing.", "(i may not being saying that quite right... i mean this like a series of odd numbers can be infinitely long - long \"in the absolute\" - without including even numbers).", "when a candidate basis for morality that's more general, like those mentioned above, is silent on an issue, or \"requires more data\", this can seem like a weakness at first glance.", "but i don't think it is a weakness:  unless we're dealing with something \"big\", with a broad context like \"humans in  this  world\" that context flexibility takes a bit of experience, data, to implement... and without that context flexibility, you end up with specific deed recommendations which (torturing infants) simply won't be absolute.", "the remaining issue i see is this:  i've mentioned a few candidates for an \"absolute basis of morality\" which are appropriately sensitive to context, but not so sensitive as to yield full-on moral relativism - we can still justify the assertion that human slavery, here, on earth, in our particular logically-possible world, is wrong.", "however, how to choose between these candidates?", "after all, you can crank up the weirdness of a thought experiment and get these candidates to give contradictory recommendations.", "they're not all right.", "i have no good answer here, but... ...this can be quickly turned into a knowledge-about-morality problem:  there could still be an absolute basis for morality, but we humans on our pale blue dot passing silently through an incredibly vast cosmos ... we may just have a particularly hard time figuring it out."], "abstract": ["if you get too specific, no.", "if you stay general and flexible, plausibly."], "candidates": [[["if you're asking about absolute answers for specific deeds:   no.", "in a logically possible world, the only way to have fun is by torturing infants.", "if the rational members of this society went behind rawls's veil of ignorance about"], 0.13627730294396959], [["if you're asking about absolute answers for specific deeds:   no.", "if your candidate absolute basis for morality gives absolute recommendations on specific deeds, there's a logically possible world that contradicts your candidate... it's not absolute.", "if you change"], 0.1866096866096866], [["if you're asking about absolute answers for specific deeds, there's a logically possible world where torture is the only way to have fun.", "in this world, the fun gained by torturing infants is extreme enough and consistent enough to outweigh any act-"], 0.10568846358320043], [["the author argues that there's a logically possible world in which torture of infants is wrong.", "in this world, the fun gained by torturing infants is extreme enough to outweigh any act- or rule-utilitarian suspicion that torturing the infants is"], 0.0], [["in a logically possible world, torturing infants is the only way to have fun.", "this is the result of rawls's veil of ignorance for \"big\" moral matters.", "in this world, regarding humans, is slavery just?", "in this logically"], 0.0], [["in a logically possible world, torturing infants is the only way to have fun.", "this is the result of rawls's veil of ignorance for \"big\" moral matters.", "in this world, regarding humans, is slavery just?"], 0.0], [["in a logically possible world, torturing infants for fun is wrong, but only if one was already tortured as an infant.", "the amount of fun one can have as an adult is proportional to the amount of torture endured as an infants."], 0.024242424242424242], [["if you're asking about absolute answers for specific deeds, there's no absolute answer.", "if you're looking for an absolute basis for morality, there is a logically possible world that contradicts your candidate.", "in this world, regarding humans is slavery"], 0.17061177815894793], [["\"it's wrong to torture infants for fun,\" says john rawls.", "but in a logically possible world, torturing infants is the only way to have fun.", "this is the result of rawls's veil of ignorance for \"big\" moral"], 0.0], [["a logically possible world where torturing infants is the only way to have fun.", "this world would justify slavery in our particular world.", "but in that world, we don't need to worry about weirdness like \"maybe they like being slaves\""], 0.0], [["john rawls: there's a logically possible world where torturing infants for fun is wrong.", "in this world, the fun gained by torturing an infant outweighs any suspicion that it's wrong.", "this world is more flexible than utilitarianism,"], 0.025641025641025644], [["answers to questions of morality are not absolute.", "if you want absolute answers for specific deeds, you need a more general basis.", "for example, it's not absolute to say it's wrong to torture infants for fun."], 0.14432439642523673], [["in a logically possible world, torturing infants is not torture.", "this is the only way one can have fun at all.", "in this world, we can justify slavery.", "this result is not absolute, but can be explained by context."], 0.0], [["in a logically possible world, torturing infants is not torture.", "this is the only way one can have fun at all.", "in this world, we can justify slavery.", "this result is not absolute, but can be explained by context.", "this article"], 0.0], [["in a logically possible world, torturing infants is not torture.", "this is the only way one can have fun at all.", "in this world, we can justify slavery."], 0.0], [["a logically possible world where torturing infants is the only way to have fun.", "this world would justify slavery in our particular world."], 0.0]], "article_untok": ["if you're asking about absolute answers for specific deeds:   no.", "what about (and i'm borrowing this from a former philosophy prof.), \"it's wrong to torture infants for fun,\" (and feel free to replace 'for fun' with 'for enjoyment' or 'to alleviate pain' or some other such thing).", "well, there's a logically possible world wherein the only way to have fun is by torturing infants.", "the infants certainly don't enjoy this, otherwise it's not torture.", "however  in this logically possible world, the only way one can have fun at all (only by torturing infants), is if one was already tortured as an infant.", "the amount of fun one can have as an adult is proportional to the amount of torture endured as an infant.", "in this logically possible world, the fun gained by torturing infants is  extreme  enough and consistent enough to outweigh any act- or rule-utilitarian suspicion that torturing the infants is wrong.", "further, one function of the torturing an individual is to facilitate their later ability to have fun, so you aren't treating the infants as a mere means.", "if the rational members of this society went behind  rawls's veil of ignorance  about having been tortured, so this passes a casual \"golden rule\" test.", "my point:  if your candidate absolute basis for morality gives absolute recommendations on specific deeds, even if it's something like \"it's wrong to torture infants for fun,\" there's a logically possible world that contradicts your candidate... it's not absolute.", "but note the standards i used for exploring the infant-torturing thought experiment... utilitarianism, kant-style deontology, the veil of ignorance, and the casual \"golden rule\".", "these have far more flexibility... any specific recommendation on a deed is context-dependent, but based outside of that context.", "if context gets weird enough, so do the specific recommendations.", "so, where does that leave us?", "well, i'm partial to rawls's veil of ignorance for \"big\" (society-wide) moral matters.", "in  this  world, regarding humans is slavery just?", "well, since we're starting with humans in  this  world, we don't need to worry about weirdness like \"maybe they like being slaves\".", "we know they don't, because we know how humans work, their basic likes and dislikes.", "so, if everyone gets behind the veil of ignorance, not knowing whether they're a slave or not, would they collectively decide that a society with slavery - one where they themselves might be slaves - is just?", "i think not.", "this (contra-slavery result) works for the other approaches i mentioned: in case you want to do a utilitarian calculation, we have a fair sense of the array of consequences of slavery.", "it's wrong.", "of course, if you change the context enough... if you put forth a sufficiently weird enough, but logically possible world, the specific answer to \"[in that logically possible world] is slavery wrong\" may differ.", "for less \"obvious\" specific recommendations, where the intuitions differ from one culture to the next (oh... i don't know... \"is it morally permissible to divorce a non-abusive spouse?\")", "you won't necessarily get a \"result\" from kant-style deontology, or utilitarianism, or rawls, from the armchair... at least not until you collect more information - enough to properly run the utilitarian calculation, imagine that society behind the veil of ignorance, or whatever.", "even then, it's probably ok for a basis for morality to be silent on some issues... it can be absolute without being all encompassing.", "(i may not being saying that quite right... i mean this like a series of odd numbers can be infinitely long - long \"in the absolute\" - without including even numbers).", "when a candidate basis for morality that's more general, like those mentioned above, is silent on an issue, or \"requires more data\", this can seem like a weakness at first glance.", "but i don't think it is a weakness:  unless we're dealing with something \"big\", with a broad context like \"humans in  this  world\" that context flexibility takes a bit of experience, data, to implement... and without that context flexibility, you end up with specific deed recommendations which (torturing infants) simply won't be absolute.", "the remaining issue i see is this:  i've mentioned a few candidates for an \"absolute basis of morality\" which are appropriately sensitive to context, but not so sensitive as to yield full-on moral relativism - we can still justify the assertion that human slavery, here, on earth, in our particular logically-possible world, is wrong.", "however, how to choose between these candidates?", "after all, you can crank up the weirdness of a thought experiment and get these candidates to give contradictory recommendations.", "they're not all right.", "i have no good answer here, but... ...this can be quickly turned into a knowledge-about-morality problem:  there could still be an absolute basis for morality, but we humans on our pale blue dot passing silently through an incredibly vast cosmos ... we may just have a particularly hard time figuring it out."], "abstract_untok": ["if you get too specific, no.", "if you stay general and flexible, plausibly."], "candidates_untok": [[["if you're asking about absolute answers for specific deeds:   no.", "in a logically possible world, the only way to have fun is by torturing infants.", "if the rational members of this society went behind rawls's veil of ignorance about"], 0.13627730294396959], [["if you're asking about absolute answers for specific deeds:   no.", "if your candidate absolute basis for morality gives absolute recommendations on specific deeds, there's a logically possible world that contradicts your candidate... it's not absolute.", "if you change"], 0.1866096866096866], [["if you're asking about absolute answers for specific deeds, there's a logically possible world where torture is the only way to have fun.", "in this world, the fun gained by torturing infants is extreme enough and consistent enough to outweigh any act-"], 0.10568846358320043], [["the author argues that there's a logically possible world in which torture of infants is wrong.", "in this world, the fun gained by torturing infants is extreme enough to outweigh any act- or rule-utilitarian suspicion that torturing the infants is"], 0.0], [["in a logically possible world, torturing infants is the only way to have fun.", "this is the result of rawls's veil of ignorance for \"big\" moral matters.", "in this world, regarding humans, is slavery just?", "in this logically"], 0.0], [["in a logically possible world, torturing infants is the only way to have fun.", "this is the result of rawls's veil of ignorance for \"big\" moral matters.", "in this world, regarding humans, is slavery just?"], 0.0], [["in a logically possible world, torturing infants for fun is wrong, but only if one was already tortured as an infant.", "the amount of fun one can have as an adult is proportional to the amount of torture endured as an infants."], 0.024242424242424242], [["if you're asking about absolute answers for specific deeds, there's no absolute answer.", "if you're looking for an absolute basis for morality, there is a logically possible world that contradicts your candidate.", "in this world, regarding humans is slavery"], 0.17061177815894793], [["\"it's wrong to torture infants for fun,\" says john rawls.", "but in a logically possible world, torturing infants is the only way to have fun.", "this is the result of rawls's veil of ignorance for \"big\" moral"], 0.0], [["a logically possible world where torturing infants is the only way to have fun.", "this world would justify slavery in our particular world.", "but in that world, we don't need to worry about weirdness like \"maybe they like being slaves\""], 0.0], [["john rawls: there's a logically possible world where torturing infants for fun is wrong.", "in this world, the fun gained by torturing an infant outweighs any suspicion that it's wrong.", "this world is more flexible than utilitarianism,"], 0.025641025641025644], [["answers to questions of morality are not absolute.", "if you want absolute answers for specific deeds, you need a more general basis.", "for example, it's not absolute to say it's wrong to torture infants for fun."], 0.14432439642523673], [["in a logically possible world, torturing infants is not torture.", "this is the only way one can have fun at all.", "in this world, we can justify slavery.", "this result is not absolute, but can be explained by context."], 0.0], [["in a logically possible world, torturing infants is not torture.", "this is the only way one can have fun at all.", "in this world, we can justify slavery.", "this result is not absolute, but can be explained by context.", "this article"], 0.0], [["in a logically possible world, torturing infants is not torture.", "this is the only way one can have fun at all.", "in this world, we can justify slavery."], 0.0], [["a logically possible world where torturing infants is the only way to have fun.", "this world would justify slavery in our particular world."], 0.0]]}