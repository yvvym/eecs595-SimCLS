import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

import pickle
def save_obj(obj, name ):
    with open('obj/'+ name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name ):
    with open('obj/' + name + '.pkl', 'rb') as f:
        return pickle.load(f)

ds = tfds.load('reddit',split='train')

d = ds.take(10000)

# T5 transformer from google https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html
import torch
import json
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config

model = T5ForConditionalGeneration.from_pretrained('t5-small')
tokenizer = T5Tokenizer.from_pretrained('t5-small')
if torch.cuda.is_available():
    device = torch.device('cuda:0')
else:
    device = torch.device('cpu')
device = 'cpu'

model = model.to(device)


data = {} #dictionary of index:[text, summarized_text]

for count, text in enumerate(d):
    print(count)
    text = text['content'].numpy().decode('utf-8')

    text = text.strip().replace("\n", "")
    t5_prepared = "summarize: " + text

    tokenized_text = tokenizer.encode(t5_prepared, return_tensors="pt").to(device)

    # summmarize 
    # with torch.no_grad():
    summary_ids = model.generate(tokenized_text,
                                        num_beams=4,
                                        no_repeat_ngram_size=2,
                                        min_length=30,
                                        max_length=100,
                                        early_stopping=True)

    out = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    text_list = [text, out]
    data[count] = text_list
    # torch.cuda.empty_cache()
